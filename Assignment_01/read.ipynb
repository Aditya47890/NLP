{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac34f03a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specified \\n as separator or delimiter. This forces the python engine which does not accept a line terminator. Hence it is not allowed to use the line terminator as separator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m reader \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_hindi.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1013\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath_or_buffer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m-> 1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   1016\u001b[0m     delim_whitespace,\n\u001b[0;32m   1017\u001b[0m     engine,\n\u001b[0;32m   1018\u001b[0m     sep,\n\u001b[0;32m   1019\u001b[0m     on_bad_lines,\n\u001b[0;32m   1020\u001b[0m     names,\n\u001b[0;32m   1021\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:2226\u001b[0m, in \u001b[0;36m_refine_defaults_read\u001b[1;34m(dialect, delimiter, delim_whitespace, engine, sep, on_bad_lines, names, defaults, dtype_backend)\u001b[0m\n\u001b[0;32m   2220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2221\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecified a delimiter with both sep and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2222\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelim_whitespace=True; you can only specify one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2223\u001b[0m     )\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delimiter \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 2226\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2227\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecified \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn as separator or delimiter. This forces the python engine \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2228\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich does not accept a line terminator. Hence it is not allowed to use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe line terminator as separator.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2230\u001b[0m     )\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delimiter \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# assign default separator value\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m delim_default\n",
      "\u001b[1;31mValueError\u001b[0m: Specified \\n as separator or delimiter. This forces the python engine which does not accept a line terminator. Hence it is not allowed to use the line terminator as separator."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reader = pd.read_csv(\"tokenized_hindi.txt\", chunksize=100000, sep=\"\\n\", header=None)\n",
    "\n",
    "# for chunk in reader:\n",
    "#     process(chunk)  # or store, write, analyze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6caf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "                                  tokenized_sentence\n",
      "0  ‡§á‡§®‡•á‡§≤‡•ã  1987 ‡§Æ‡•á‡§Ç  ‡§â‡§∏  ‡§µ‡§ï‡•ç‡§§  ‡§ê‡§∏‡•á  ‡§π‡•Ä  ‡§¶‡•ã‡§∞‡§æ‡§π‡•á  ‡§™‡§∞...\n",
      "1  ‡§π‡§æ‡§≤‡§æ‡§Ç‡§ï‡§ø  ‡§§‡§¨  ‡§™‡§æ‡§∞‡•ç‡§ü‡•Ä  ‡§™‡§∞  ‡§¶‡•á‡§µ‡•Ä‡§≤‡§æ‡§≤  ‡§ï‡•Ä  ‡§Æ‡§ú‡§¨‡•Ç‡§§  ‡§™...\n",
      "2  1989 ‡§Æ‡•á‡§Ç  ‡§¶‡•á‡§µ‡•Ä‡§≤‡§æ‡§≤  ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞  ‡§ï‡•Ä  ‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø  ‡§Æ‡•á‡§Ç  ...\n",
      "3  ‡§â‡§®  ‡§™‡§∞‡§ø‡§∏‡•ç‡§•‡§ø‡§§‡§ø‡§Ø‡•ã‡§Ç  ‡§Æ‡•á‡§Ç  ‡§¶‡•á‡§µ‡•Ä‡§≤‡§æ‡§≤  ‡§®‡•á  ‡§ï‡§°‡§º‡§æ  ‡§®‡§ø‡§∞‡•ç...\n",
      "4  ‡§â‡§∏  ‡§∏‡§Æ‡§Ø  ‡§∞‡§£‡§ú‡•Ä‡§§  ‡§ï‡•Ä  ‡§®‡§æ‡§∞‡§æ‡§ú‡§ó‡•Ä  ‡§ï‡•á  ‡§ö‡§≤‡§§‡•á  ‡§â‡§®‡§ï‡•á  ‡§∏...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import islice\n",
    "\n",
    "# File path\n",
    "file_path = \"C:\\\\Users\\\\adity\\\\Desktop\\\\NLP\\\\tokenized_hindi.txt\"\n",
    "\n",
    "# Number of lines to read\n",
    "num_lines = 500_000\n",
    "\n",
    "# Read 5 lakh lines into a DataFrame\n",
    "with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "    lines = list(islice(file, num_lines))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(lines, columns=[\"tokenized_sentence\"])\n",
    "\n",
    "# Strip any leading/trailing whitespace\n",
    "df[\"tokenized_sentence\"] = df[\"tokenized_sentence\"].str.strip()\n",
    "\n",
    "print(\"Done\")\n",
    "# Show preview\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee2a41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                        tokenized_sentence\n",
       "0       ‡§á‡§®‡•á‡§≤‡•ã  1987 ‡§Æ‡•á‡§Ç  ‡§â‡§∏  ‡§µ‡§ï‡•ç‡§§  ‡§ê‡§∏‡•á  ‡§π‡•Ä  ‡§¶‡•ã‡§∞‡§æ‡§π‡•á  ‡§™‡§∞...\n",
       "1       ‡§π‡§æ‡§≤‡§æ‡§Ç‡§ï‡§ø  ‡§§‡§¨  ‡§™‡§æ‡§∞‡•ç‡§ü‡•Ä  ‡§™‡§∞  ‡§¶‡•á‡§µ‡•Ä‡§≤‡§æ‡§≤  ‡§ï‡•Ä  ‡§Æ‡§ú‡§¨‡•Ç‡§§  ‡§™...\n",
       "2       1989 ‡§Æ‡•á‡§Ç  ‡§¶‡•á‡§µ‡•Ä‡§≤‡§æ‡§≤  ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞  ‡§ï‡•Ä  ‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø  ‡§Æ‡•á‡§Ç  ...\n",
       "3       ‡§â‡§®  ‡§™‡§∞‡§ø‡§∏‡•ç‡§•‡§ø‡§§‡§ø‡§Ø‡•ã‡§Ç  ‡§Æ‡•á‡§Ç  ‡§¶‡•á‡§µ‡•Ä‡§≤‡§æ‡§≤  ‡§®‡•á  ‡§ï‡§°‡§º‡§æ  ‡§®‡§ø‡§∞‡•ç...\n",
       "4       ‡§â‡§∏  ‡§∏‡§Æ‡§Ø  ‡§∞‡§£‡§ú‡•Ä‡§§  ‡§ï‡•Ä  ‡§®‡§æ‡§∞‡§æ‡§ú‡§ó‡•Ä  ‡§ï‡•á  ‡§ö‡§≤‡§§‡•á  ‡§â‡§®‡§ï‡•á  ‡§∏...\n",
       "...                                                   ...\n",
       "499995  ‡§Æ‡•ã‡§π‡§Æ‡•ç‡§Æ‡§¶  ‡§´‡§µ‡§æ‡§¶  ‡§´‡§æ‡§∞‡•Ç‡§ñ  ‡§ñ‡§æ‡§®  ‡§ú‡•á‡§è‡§®  ‡§Æ‡•á‡§°‡§ø‡§ï‡§≤  ‡§ï‡•â‡§≤‡•á‡§ú...\n",
       "499996  ‡§µ‡•á  ‡§Ö‡§™‡§®‡•á  ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞  ‡§ï‡•á  ‡§∏‡§æ‡§•  ‡§Æ‡§Ç‡§ú‡•Ç‡§∞‡§ó‡§¢‡§º‡•Ä  ‡§ï‡•á  ‡§™‡§æ‡§Æ ...\n",
       "499997   ‡§â‡§®‡§ï‡•á  ‡§≠‡§æ‡§à  ‡§≠‡•Ä  ‡§Æ‡•á‡§°‡§ø‡§ï‡§≤  ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞  ‡§∏‡•á  ‡§ú‡•Å‡§°‡§º‡•á  ‡§π‡•à‡§Ç ‡•§\n",
       "499998  ‡§Æ‡•ã‡§π‡§Æ‡•ç‡§Æ‡§¶  ‡§´‡§µ‡§æ‡§¶  ‡§ï‡§æ  ‡§ï‡§π‡§®‡§æ  ‡§π‡•à  ‡§ï‡§ø  ‡§â‡§®‡§ï‡•Ä  ‡§∏‡•ã‡§∏‡§æ‡§Ø‡§ü‡•Ä...\n",
       "499999  ‡§≤‡•ã‡§ó‡•ã‡§Ç  ‡§ï‡•ã  ‡§≤‡§ó‡§§‡§æ  ‡§π‡•à  ‡§ï‡§ø  ‡§â‡§®‡§ï‡•á  ‡§ú‡§∞‡§ø‡§è  ‡§∏‡•ã‡§∏‡§æ‡§Ø‡§ü‡•Ä  ...\n",
       "\n",
       "[500000 rows x 1 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0389ac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500000 entries, 0 to 499999\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   tokenized_sentence  500000 non-null  object\n",
      " 1   tokens              500000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe6346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>455839</th>\n",
       "      <td>‡§™‡§π‡§≤‡•á  ‡§™‡§ï‡•ç‡§∑  ‡§ï‡§æ  ‡§Ü‡§∞‡•ã‡§™  ‡§π‡•à  ‡§ï‡§ø  ‡§™‡•Å‡§≤‡§ø‡§∏  ‡§®‡•á  ‡§™‡§ï‡•ú‡•á ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172211</th>\n",
       "      <td>‡§´‡•á‡§≤‡•ç‡§™‡•ç‡§∏  ‡§®‡•á  ‡§ü‡•Ç‡§∞‡•ç‡§®‡§æ‡§Æ‡•á‡§Ç‡§ü  ‡§Æ‡•á‡§Ç  ‡§§‡•Ä‡§∏‡§∞‡•Ä  ‡§¨‡§æ‡§∞  ‡§Ø‡§π  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381296</th>\n",
       "      <td>‡§Ö‡§ï‡•ç‡§ü‡•Ç‡§¨‡§∞  ‡§ï‡•á  ‡§Æ‡§π‡•Ä‡§®‡•á  ‡§Æ‡•á‡§Ç  ‡§Ü‡§™‡§ï‡•Ä  ‡§Ø‡•ã‡§ú‡§®‡§æ‡§ì‡§Ç  ‡§ï‡•ã  ‡§Ü‡§ï...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420611</th>\n",
       "      <td>‡§¨‡•à‡§†‡§ï  ‡§Æ‡•á‡§Ç  ‡§ú‡§®‡§ï‡§≤‡•ç‡§Ø‡§æ‡§£‡§ï‡§æ‡§∞‡•Ä  ‡§Ø‡•ã‡§ú‡§®‡§æ‡§ì‡§Ç  ‡§ï‡•á  ‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ‡§®‡•ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17994</th>\n",
       "      <td>ii .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80889</th>\n",
       "      <td>‡§ï‡§æ‡§∞‡•ç‡§Ø‡§∏‡§Æ‡§ø‡§§‡§ø  ‡§Æ‡•á‡§Ç  ‡§¨‡§∞‡§ï‡§∞‡§æ‡§∞  ‡§∞‡§π‡§®‡•á  ‡§µ‡§æ‡§≤‡•ã‡§Ç  ‡§Æ‡•á‡§Ç  ‡§∞‡§æ‡§π...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103799</th>\n",
       "      <td>‡§¨‡•Å‡§ß‡§µ‡§æ‡§∞  ‡§∏‡•Å‡§¨‡§π  ‡§ö‡§æ‡§∞  ‡§¨‡§ú‡•á  ‡§Æ‡§ï‡§æ‡§®  ‡§ï‡•á  ‡§è‡§ï  ‡§π‡§ø‡§∏‡•ç‡§∏‡•á  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187648</th>\n",
       "      <td>‡§Æ‡•Å‡§†‡§≠‡•á‡•ú  ‡§Æ‡•á‡§Ç  ‡§§‡§∏‡•ç‡§ï‡§∞‡•ã‡§Ç  ‡§î‡§∞  ‡§™‡•Å‡§≤‡§ø‡§∏  ‡§ï‡•Ä  ‡§§‡§∞‡§´  ‡§ï‡§à  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317243</th>\n",
       "      <td>‡§¨‡§ó‡§æ‡§µ‡§§  ‡§ï‡•á  ‡§§‡•Å‡§∞‡§Ç‡§§  ‡§¨‡§æ‡§¶  ‡§µ‡•á  ‡§™‡§æ‡§Ç‡§ö  ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï‡•ã‡§Ç  ‡§ï‡•á ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184405</th>\n",
       "      <td>‡§µ‡§π  ‡§Ü‡§™‡§ï‡•ã  ‡§Ü‡§™‡§ï‡•á  ‡§¶‡•ã‡§∏‡•ç‡§§‡•ã‡§Ç , ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞  ‡§ï‡•á  ‡§∏‡§æ‡§•  ‡§µ‡§ï‡•ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375555</th>\n",
       "      <td>‡§á‡§∏‡§ï‡•á  ‡§Ö‡§≤‡§æ‡§µ‡§æ  ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡§ï‡•ç‡§∑  ‡§ï‡§∞  ‡§ï‡§æ‡§®‡•Ç‡§®  ‡§ï‡•ã  ‡§∏‡§∞‡§≤  ‡§¨‡§®...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498558</th>\n",
       "      <td>‡§Ö‡§≠‡§ø‡§Ø‡•Å‡§ï‡•ç‡§§  ‡§µ‡§ø‡§ï‡§æ‡§∏  ‡§¶‡•Å‡§¨‡•á  ‡§è‡§µ‡§Ç  ‡§â‡§∏‡§ï‡•á  ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç  ‡§ï‡•á ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46061</th>\n",
       "      <td>‡§∞‡§æ‡§π‡•Å‡§ï‡§æ‡§≤: ‡§™‡•ç‡§∞‡§æ‡§§: ‡•Ø.‡•¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121255</th>\n",
       "      <td>‡§Ø‡§π  ‡§ï‡§∞‡•ã  ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø  ‡§§‡•Å‡§Æ  ‡§á‡§∏‡•á  ‡§™‡•ç‡§Ø‡§æ‡§∞  ‡§ï‡§∞‡§§‡•á  ‡§π‡•ã , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92989</th>\n",
       "      <td>‡§Ø‡§π  ‡§§‡•ã  ‡§ú‡§®‡§§‡§æ  ‡§π‡•Ä  ‡§¨‡§§‡§æ‡§è‡§ó‡•Ä  ‡§≤‡•á‡§ï‡§ø‡§®  ‡§â‡§®‡§ï‡•á  ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ï ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439632</th>\n",
       "      <td>‡§≠‡•ã‡§™‡§æ‡§≤  ‡§°‡•á‡§≤‡•Ä  ‡§π‡§ø‡§Ç‡§¶‡•Ä  ‡§®‡•ç‚Äç ‡§Ø‡•Ç‡§ú‡§º) ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141515</th>\n",
       "      <td>‡§°‡•â .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458794</th>\n",
       "      <td>‡§≠‡•ã‡§™‡§æ‡§≤ ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107668</th>\n",
       "      <td>‡§î‡§∞  ‡§á‡§∏‡•Ä  ‡§∏‡•Ç‡§ö‡•Ä  ‡§Æ‡•á‡§Ç  ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ  ‡§ï‡•á  ‡§®‡•á‡§§‡§æ‡§ì‡§Ç  ‡§ï‡•Ä  ‡§∏‡•Ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>‡§Ü‡§∞‡•ç‡§ö‡§∞  ‡§ê‡§∏‡•á  ‡§π‡•Ä  ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä  ‡§π‡•à‡§Ç .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201633</th>\n",
       "      <td>‡§™‡•Å‡§≤‡§ø‡§∏  ‡§®‡•á  ‡§∏‡§≠‡•Ä  ‡§õ‡§æ‡§§‡•ç‡§∞  ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®‡§æ‡§∞‡§ø‡§Ø‡•ã‡§Ç  ‡§ï‡•ã  ‡§ï‡§∞‡§®...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420318</th>\n",
       "      <td>‡§¨‡§§‡§æ‡§Ø‡§æ  ‡§ú‡§æ  ‡§∞‡§π‡§æ  ‡§π‡•à  ‡§ï‡§ø  ‡§¶‡•ç‡§∞‡•ã‡§£‡§æ‡§ö‡§æ‡§∞‡•ç‡§Ø  ‡§Ö‡§µ‡•â‡§∞‡•ç‡§°  ‡§∏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119708</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455097</th>\n",
       "      <td>‡§ï‡§∞‡§õ‡§®‡§æ  ‡§•‡§æ‡§®‡§æ  ‡§ï‡•á  ‡§≤‡•ã‡§π‡§¶‡•Ä  ‡§ó‡§æ‡§Ç‡§µ  ‡§®‡§ø‡§µ‡§æ‡§∏‡•Ä  ‡§Æ‡§π‡•á‡§®‡•ç‡§¶‡•ç‡§∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425043</th>\n",
       "      <td>‡§Ø‡•á  ‡§õ‡§æ‡§§‡•ç‡§∞  ‡§¨‡§ø‡§π‡§æ‡§∞  ‡§Æ‡•á‡§Ç  ‡§™‡§ü‡§®‡§æ , ‡§ó‡§Ø‡§æ , ‡§∏‡§Æ‡§∏‡•ç‡§§‡•Ä‡§™‡•Å‡§∞ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tokenized_sentence\n",
       "455839  ‡§™‡§π‡§≤‡•á  ‡§™‡§ï‡•ç‡§∑  ‡§ï‡§æ  ‡§Ü‡§∞‡•ã‡§™  ‡§π‡•à  ‡§ï‡§ø  ‡§™‡•Å‡§≤‡§ø‡§∏  ‡§®‡•á  ‡§™‡§ï‡•ú‡•á ...\n",
       "172211  ‡§´‡•á‡§≤‡•ç‡§™‡•ç‡§∏  ‡§®‡•á  ‡§ü‡•Ç‡§∞‡•ç‡§®‡§æ‡§Æ‡•á‡§Ç‡§ü  ‡§Æ‡•á‡§Ç  ‡§§‡•Ä‡§∏‡§∞‡•Ä  ‡§¨‡§æ‡§∞  ‡§Ø‡§π  ...\n",
       "381296  ‡§Ö‡§ï‡•ç‡§ü‡•Ç‡§¨‡§∞  ‡§ï‡•á  ‡§Æ‡§π‡•Ä‡§®‡•á  ‡§Æ‡•á‡§Ç  ‡§Ü‡§™‡§ï‡•Ä  ‡§Ø‡•ã‡§ú‡§®‡§æ‡§ì‡§Ç  ‡§ï‡•ã  ‡§Ü‡§ï...\n",
       "420611  ‡§¨‡•à‡§†‡§ï  ‡§Æ‡•á‡§Ç  ‡§ú‡§®‡§ï‡§≤‡•ç‡§Ø‡§æ‡§£‡§ï‡§æ‡§∞‡•Ä  ‡§Ø‡•ã‡§ú‡§®‡§æ‡§ì‡§Ç  ‡§ï‡•á  ‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ‡§®‡•ç...\n",
       "17994                                                ii .\n",
       "80889   ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§∏‡§Æ‡§ø‡§§‡§ø  ‡§Æ‡•á‡§Ç  ‡§¨‡§∞‡§ï‡§∞‡§æ‡§∞  ‡§∞‡§π‡§®‡•á  ‡§µ‡§æ‡§≤‡•ã‡§Ç  ‡§Æ‡•á‡§Ç  ‡§∞‡§æ‡§π...\n",
       "103799  ‡§¨‡•Å‡§ß‡§µ‡§æ‡§∞  ‡§∏‡•Å‡§¨‡§π  ‡§ö‡§æ‡§∞  ‡§¨‡§ú‡•á  ‡§Æ‡§ï‡§æ‡§®  ‡§ï‡•á  ‡§è‡§ï  ‡§π‡§ø‡§∏‡•ç‡§∏‡•á  ...\n",
       "187648  ‡§Æ‡•Å‡§†‡§≠‡•á‡•ú  ‡§Æ‡•á‡§Ç  ‡§§‡§∏‡•ç‡§ï‡§∞‡•ã‡§Ç  ‡§î‡§∞  ‡§™‡•Å‡§≤‡§ø‡§∏  ‡§ï‡•Ä  ‡§§‡§∞‡§´  ‡§ï‡§à  ...\n",
       "317243  ‡§¨‡§ó‡§æ‡§µ‡§§  ‡§ï‡•á  ‡§§‡•Å‡§∞‡§Ç‡§§  ‡§¨‡§æ‡§¶  ‡§µ‡•á  ‡§™‡§æ‡§Ç‡§ö  ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï‡•ã‡§Ç  ‡§ï‡•á ...\n",
       "184405  ‡§µ‡§π  ‡§Ü‡§™‡§ï‡•ã  ‡§Ü‡§™‡§ï‡•á  ‡§¶‡•ã‡§∏‡•ç‡§§‡•ã‡§Ç , ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞  ‡§ï‡•á  ‡§∏‡§æ‡§•  ‡§µ‡§ï‡•ç...\n",
       "375555  ‡§á‡§∏‡§ï‡•á  ‡§Ö‡§≤‡§æ‡§µ‡§æ  ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡§ï‡•ç‡§∑  ‡§ï‡§∞  ‡§ï‡§æ‡§®‡•Ç‡§®  ‡§ï‡•ã  ‡§∏‡§∞‡§≤  ‡§¨‡§®...\n",
       "498558  ‡§Ö‡§≠‡§ø‡§Ø‡•Å‡§ï‡•ç‡§§  ‡§µ‡§ø‡§ï‡§æ‡§∏  ‡§¶‡•Å‡§¨‡•á  ‡§è‡§µ‡§Ç  ‡§â‡§∏‡§ï‡•á  ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç  ‡§ï‡•á ...\n",
       "46061                                 ‡§∞‡§æ‡§π‡•Å‡§ï‡§æ‡§≤: ‡§™‡•ç‡§∞‡§æ‡§§: ‡•Ø.‡•¶\n",
       "121255  ‡§Ø‡§π  ‡§ï‡§∞‡•ã  ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø  ‡§§‡•Å‡§Æ  ‡§á‡§∏‡•á  ‡§™‡•ç‡§Ø‡§æ‡§∞  ‡§ï‡§∞‡§§‡•á  ‡§π‡•ã , ...\n",
       "92989   ‡§Ø‡§π  ‡§§‡•ã  ‡§ú‡§®‡§§‡§æ  ‡§π‡•Ä  ‡§¨‡§§‡§æ‡§è‡§ó‡•Ä  ‡§≤‡•á‡§ï‡§ø‡§®  ‡§â‡§®‡§ï‡•á  ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ï ...\n",
       "439632                    ‡§≠‡•ã‡§™‡§æ‡§≤  ‡§°‡•á‡§≤‡•Ä  ‡§π‡§ø‡§Ç‡§¶‡•Ä  ‡§®‡•ç‚Äç ‡§Ø‡•Ç‡§ú‡§º) ‡•§\n",
       "141515                                               ‡§°‡•â .\n",
       "458794                                            ‡§≠‡•ã‡§™‡§æ‡§≤ ‡•§\n",
       "107668  ‡§î‡§∞  ‡§á‡§∏‡•Ä  ‡§∏‡•Ç‡§ö‡•Ä  ‡§Æ‡•á‡§Ç  ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ  ‡§ï‡•á  ‡§®‡•á‡§§‡§æ‡§ì‡§Ç  ‡§ï‡•Ä  ‡§∏‡•Ç...\n",
       "2099                       ‡§Ü‡§∞‡•ç‡§ö‡§∞  ‡§ê‡§∏‡•á  ‡§π‡•Ä  ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä  ‡§π‡•à‡§Ç .\n",
       "201633  ‡§™‡•Å‡§≤‡§ø‡§∏  ‡§®‡•á  ‡§∏‡§≠‡•Ä  ‡§õ‡§æ‡§§‡•ç‡§∞  ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®‡§æ‡§∞‡§ø‡§Ø‡•ã‡§Ç  ‡§ï‡•ã  ‡§ï‡§∞‡§®...\n",
       "420318  ‡§¨‡§§‡§æ‡§Ø‡§æ  ‡§ú‡§æ  ‡§∞‡§π‡§æ  ‡§π‡•à  ‡§ï‡§ø  ‡§¶‡•ç‡§∞‡•ã‡§£‡§æ‡§ö‡§æ‡§∞‡•ç‡§Ø  ‡§Ö‡§µ‡•â‡§∞‡•ç‡§°  ‡§∏...\n",
       "119708                                                  .\n",
       "455097  ‡§ï‡§∞‡§õ‡§®‡§æ  ‡§•‡§æ‡§®‡§æ  ‡§ï‡•á  ‡§≤‡•ã‡§π‡§¶‡•Ä  ‡§ó‡§æ‡§Ç‡§µ  ‡§®‡§ø‡§µ‡§æ‡§∏‡•Ä  ‡§Æ‡§π‡•á‡§®‡•ç‡§¶‡•ç‡§∞...\n",
       "425043  ‡§Ø‡•á  ‡§õ‡§æ‡§§‡•ç‡§∞  ‡§¨‡§ø‡§π‡§æ‡§∞  ‡§Æ‡•á‡§Ç  ‡§™‡§ü‡§®‡§æ , ‡§ó‡§Ø‡§æ , ‡§∏‡§Æ‡§∏‡•ç‡§§‡•Ä‡§™‡•Å‡§∞ ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfe901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Statistics:\n",
      "1. Total Sentences: 500000\n",
      "2. Total Words: 9474163\n",
      "3. Total Characters: 35303216\n",
      "4. Average Sentence Length: 18.95 words/sentence\n",
      "5. Average Word Length: 3.73 characters/word\n",
      "6. Type/Token Ratio (TTR): 0.0216\n"
     ]
    }
   ],
   "source": [
    "# Convert all sentences into lists of words\n",
    "df[\"tokens\"] = df[\"tokenized_sentence\"].str.split()\n",
    "\n",
    "# i. Total number of sentences\n",
    "total_sentences = len(df)\n",
    "\n",
    "# ii. Total number of words\n",
    "total_words = df[\"tokens\"].apply(len).sum()\n",
    "\n",
    "# iii. Total number of characters (excluding spaces)\n",
    "total_characters = df[\"tokens\"].apply(lambda tokens: sum(len(token) for token in tokens)).sum()\n",
    "\n",
    "# iv. Average Sentence Length (words per sentence)\n",
    "avg_sentence_length = total_words / total_sentences\n",
    "\n",
    "# v. Average Word Length (characters per word)\n",
    "avg_word_length = total_characters / total_words\n",
    "\n",
    "# vi. Type/Token Ratio (TTR)\n",
    "all_tokens = [token for tokens in df[\"tokens\"] for token in tokens]\n",
    "unique_tokens = set(all_tokens)\n",
    "ttr = len(unique_tokens) / total_words\n",
    "\n",
    "# üßæ Final Output\n",
    "print(\"Corpus Statistics:\")\n",
    "print(f\"1. Total Sentences: {total_sentences}\")\n",
    "print(f\"2. Total Words: {total_words}\")\n",
    "print(f\"3. Total Characters: {total_characters}\")\n",
    "print(f\"4. Average Sentence Length: {avg_sentence_length:.2f} words/sentence\")\n",
    "print(f\"5. Average Word Length: {avg_word_length:.2f} characters/word\")\n",
    "print(f\"6. Type/Token Ratio (TTR): {ttr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722ee1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ba848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
